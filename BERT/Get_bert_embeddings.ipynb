{
 "cells":[
  {
   "cell_type":"code",
   "source":[
    "!pip install pyarrow"
   ],
   "execution_count":1,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Collecting pyarrow\r\n",
      "  Downloading pyarrow-15.0.0-cp38-cp38-manylinux_2_28_x86_64.whl (38.4 MB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0\/38.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0\/38.4 MB\u001b[0m \u001b[31m117.8 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1\/38.4 MB\u001b[0m \u001b[31m116.2 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5\/38.4 MB\u001b[0m \u001b[31m119.1 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9\/38.4 MB\u001b[0m \u001b[31m118.4 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1\/38.4 MB\u001b[0m \u001b[31m120.0 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m25.5\/38.4 MB\u001b[0m \u001b[31m120.2 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m29.8\/38.4 MB\u001b[0m \u001b[31m122.6 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m32.2\/38.4 MB\u001b[0m \u001b[31m121.9 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m35.6\/38.4 MB\u001b[0m \u001b[31m94.4 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m38.4\/38.4 MB\u001b[0m \u001b[31m95.1 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m38.4\/38.4 MB\u001b[0m \u001b[31m95.1 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m38.4\/38.4 MB\u001b[0m \u001b[31m95.1 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m38.4\/38.4 MB\u001b[0m \u001b[31m95.1 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4\/38.4 MB\u001b[0m \u001b[31m36.5 MB\/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1.16.6 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from pyarrow) (1.24.3)\r\n",
      "Installing collected packages: pyarrow\r\n",
      "Successfully installed pyarrow-15.0.0\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"N5SljJc4JiCOpfVlFiMlwi",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"7NLQSGlxCXaBLkcwLPKcA3"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import re\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "import os"
   ],
   "execution_count":2,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"HGJpwSfhckY2fiG3dclstM",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"FdQEykBKwaHEICXVqGXXyl"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# # Load the pre-trained BERT model and tokenizer\n",
    "# device = \"cpu\"\n",
    "# device = \"cuda:0\"\n",
    "# model_name = 'bert-base-uncased'  # You can use other pre-trained BERT models\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# model = BertModel.from_pretrained(model_name).to(device)"
   ],
   "execution_count":3,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"FyZrkwQA1VOcCMQylORYNF",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"XHslOzuSXXATF7X9WrARGM"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "'''print(tokens)\n",
    "    return tokens\n",
    "    if len(tokens) > 510:\n",
    "        print(\"Exceeded token length\")\n",
    "        tokens = tokens[:510]\n",
    "\n",
    "    # Add the special [CLS] and [SEP] tokens\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "\n",
    "    # Convert tokens to token IDs\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)'''\n",
    "\n",
    "     #print(\"Shape of embeddings array:\", token_embeddings.shape)\n",
    "\n",
    "\n",
    "    # Extract the embeddings for each token (excluding [CLS] and [SEP])\n",
    "    #token_embeddings = token_embeddings[0][1:-1]\n",
    "\n",
    "    #word_embeddings = token_embeddings.cpu().numpy()\n",
    "    # Average the word embeddings to obtain document embeddings\n",
    "    #document_embedding_mean = torch.sum(token_embeddings, dim=0).cpu().numpy()"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"z6TIxntz1FAJSVoBOIj7K9",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def get_embeddings_for_doc(text):\n",
    "    # Tokenize the text\n",
    "    token_ids = tokenizer(text, padding = True)['input_ids']\n",
    "    \n",
    "    # Convert token IDs to tensor\n",
    "    tokens_tensor = torch.tensor(token_ids).to(device)\n",
    "    if tokens_tensor.shape[1] > 510:\n",
    "        tokens_tensor = torch.cat((tokens_tensor[..., :510], torch.tensor([[102]], device=device).repeat(tokens_tensor.shape[0], 1)), -1)\n",
    "\n",
    "    # Get the embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        #token_embeddings = outputs['last_hidden_state']  # Output embeddings from the model\n",
    "        document_embedding = outputs['pooler_output']\n",
    "\n",
    "    # Convert tensor to numpy array\n",
    "    document_embedding = document_embedding.cpu().numpy().reshape(-1,768).sum(0)\n",
    "\n",
    "\n",
    "    # Print the shape of the word embeddings and document embedding\n",
    "    #print(\"Shape of doc mean embeddings:\", document_embedding_mean.shape)\n",
    "    #print(\"Shape of document embedding:\", document_embedding.shape)\n",
    "    return document_embedding\n"
   ],
   "execution_count":4,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"lHEDfkd8y0iqnp6XLGwXcp",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"jRv40eEutC7TJi2EHfYnLD"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# a = get_embeddings_for_doc(['I am happy', 'I am very happy a a a a a a'])\n",
    "# a.shape"
   ],
   "execution_count":5,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"WJofGrjJapU5B0fMw3J1gL",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# torch.tensor([[102]]).repeat(10, 1).shape"
   ],
   "execution_count":6,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"fC10iocFdL8WnyixXEBI5c",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# torch.cuda.is_available()"
   ],
   "execution_count":7,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"TORT4F5blUU43wKUZe5mSh",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# a = get_embeddings_for_doc(['I am happy', 'I am very happy'])\n",
    "# a.shape"
   ],
   "execution_count":8,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"46X28DtkPHDQvNTLGN8bdu",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# get file path as dict[int, list[str]] where \n",
    "# key is the year and value is the list of file paths\n",
    "\n",
    "# break the text into itemized sections\n",
    "def get_itemized_10k(fname, sections=['business', 'risk', 'mda', 'qmr']):\n",
    "    '''Extract ITEM from 10k filing text.\n",
    "\n",
    "    Args:\n",
    "        fname: str, the file name (ends with .txt)\n",
    "        sections: list of sections to extract\n",
    "\n",
    "    Returns:\n",
    "        itemized_text: dict[str, str], where key is the section name and value is the text\n",
    "    '''\n",
    "    \n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    def extract_text(text, item_start, item_end):\n",
    "        '''\n",
    "        Args:\n",
    "            text: 10K filing text\n",
    "            item_start: compiled regex pattern\n",
    "            item_end: compiled regex pattern\n",
    "        '''\n",
    "        item_start = item_start\n",
    "        item_end = item_end\n",
    "        starts = [i.start() for i in item_start.finditer(text)]\n",
    "        ends = [i.start() for i in item_end.finditer(text)]\n",
    "\n",
    "        # if no matches, return empty string\n",
    "        if len(starts) == 0 or len(ends) == 0:\n",
    "            return None\n",
    "\n",
    "        # get possible start\/end positions\n",
    "        # we may end up with multiple start\/end positions, and we'll choose the longest\n",
    "        # item text.\n",
    "        positions = list()\n",
    "        for s in starts:\n",
    "            control = 0\n",
    "            for e in ends:\n",
    "                if control == 0:\n",
    "                    if s < e:\n",
    "                        control = 1\n",
    "                        positions.append([s,e])\n",
    "\n",
    "        # get the longest item text\n",
    "        item_length = 0\n",
    "        item_position = list()\n",
    "        for p in positions:\n",
    "            if (p[1]-p[0]) > item_length:\n",
    "                item_length = p[1]-p[0]\n",
    "                item_position = p\n",
    "\n",
    "        item_text = text[item_position[0]:item_position[1]]\n",
    "\n",
    "        return item_text\n",
    "\n",
    "    # extract text for each section\n",
    "    results = {}\n",
    "\n",
    "    for section in sections:\n",
    "\n",
    "        # ITEM 1: Business\n",
    "        # if there's no ITEM 1A then it ends at ITEM 2\n",
    "        if section == 'business':\n",
    "            try:\n",
    "                item1_start = re.compile(\"i\\s?tem[s]?\\s*[1I]\\s*[\\.\\;\\:\\-\\_]*\\s*\\\\b|PART\\s*[1I]\\s\\s*[\\.\\;\\:\\-\\_]*\\s*\", re.IGNORECASE)\n",
    "                item1_end = re.compile(\"item\\s*1a\\s*[\\.\\;\\:\\-\\_]*\\s*Risk|item\\s*2\\s*[\\.\\,\\;\\:\\-\\_]*\\s*(Desc|Prop)\", re.IGNORECASE)\n",
    "                business_text = extract_text(text, item1_start, item1_end)\n",
    "                results['business'] = business_text\n",
    "            except Exception as e:\n",
    "                results['business'] = ''\n",
    "                print(f'Error extracting ITEM 1: Business for {fname}')\n",
    "            \n",
    "        # ITEM 1A: Risk Factors\n",
    "        # it ends at ITEM 2\n",
    "        if section == 'risk':\n",
    "            try:\n",
    "                item1a_start = re.compile(\"(?<!,\\s)item\\s*1a[\\.\\;\\:\\-\\_]*\\s*Risk\", re.IGNORECASE)\n",
    "                item1a_end = re.compile(\"item\\s*2\\s*[\\.\\;\\:\\-\\_]*\\s*(Desc|Prop)|item\\s*[1I]\\s*[\\.\\;\\:\\-\\_]*\\s*\\\\b\", re.IGNORECASE)\n",
    "                risk_text = extract_text(text, item1a_start, item1a_end)\n",
    "                results['risk'] = risk_text\n",
    "            except Exception as e:\n",
    "                results['risk'] = ''\n",
    "                print(f'Error extracting ITEM 1A: Risk Factors for {fname}')\n",
    "                \n",
    "        # ITEM 7: Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
    "        # it ends at ITEM 7A (if it exists) or ITEM 8\n",
    "        if section == 'mda':\n",
    "            try:\n",
    "                item7_start = re.compile(\"item\\s*7\\s*[\\.\\;\\:\\-\\_]*\\s*\\\\bM\", re.IGNORECASE)\n",
    "                item7_end = re.compile(\"item\\s*7a\\s*[\\.\\;\\:\\-\\_]*[\\s\\n]*Quanti|item\\s*8\\s*[\\.\\,\\;\\:\\-\\_]*\\s*Finan\", re.IGNORECASE)\n",
    "                item7_text = extract_text(text, item7_start, item7_end)\n",
    "                results['mda'] = item7_text\n",
    "            except Exception as e:\n",
    "                results['mda'] = ''\n",
    "                print(f'Error extracting ITEM 7: MD&A for {fname}')\n",
    "\n",
    "        # ITEM 7A: Quantitative and Qualitative Disclosures About Market Risk\n",
    "        # \n",
    "        if section == 'qmr':\n",
    "            try:\n",
    "                item7a_start = re.compile(\"item\\s*7a\\s*[\\.\\;\\:\\-\\_]*[\\s\\n]*Quanti\", re.IGNORECASE)\n",
    "                item7a_end = re.compile(\"item\\s*8\\s*[\\.\\,\\;\\:\\-\\_]*\\s*Finan\", re.IGNORECASE)\n",
    "                item7a_text = extract_text(text, item7a_start, item7a_end)\n",
    "                results['qmr'] = item7a_text\n",
    "            except Exception as e:\n",
    "                results['qmr'] = ''\n",
    "                print(f'Error extracting ITEM 7A: for {fname}')\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def get_itemized_10q(fname, sections=['mda', 'qmr']):\n",
    "    '''Extract ITEM from 10q filing text.\n",
    "    Args:\n",
    "        fname: str, the file name (ends with .txt)\n",
    "        sections: list of sections to extract\n",
    "    Returns:\n",
    "        itemized_text: dict[str, str], where key is the section name and value is the text\n",
    "    '''\n",
    "    \n",
    "    with open(fname, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    def extract_text(text, item_start, item_end):\n",
    "        '''\n",
    "        Args:\n",
    "            text: 10q filing text\n",
    "            item_start: compiled regex pattern\n",
    "            item_end: compiled regex pattern\n",
    "        '''\n",
    "        item_start = item_start\n",
    "        item_end = item_end\n",
    "        starts = [i.start() for i in item_start.finditer(text)]\n",
    "        ends = [i.start() for i in item_end.finditer(text)]\n",
    "\n",
    "        # if no matches, return empty string\n",
    "        if len(starts) == 0 or len(ends) == 0:\n",
    "            return None\n",
    "\n",
    "        # get possible start\/end positions\n",
    "        # we may end up with multiple start\/end positions, and we'll choose the longest\n",
    "        # item text.\n",
    "        positions = list()\n",
    "        for s in starts:\n",
    "            control = 0\n",
    "            for e in ends:\n",
    "                if control == 0:\n",
    "                    if s < e:\n",
    "                        control = 1\n",
    "                        positions.append([s,e])\n",
    "\n",
    "        # get the longest item text\n",
    "        item_length = 0\n",
    "        item_position = list()\n",
    "        for p in positions:\n",
    "            if (p[1]-p[0]) > item_length:\n",
    "                item_length = p[1]-p[0]\n",
    "                item_position = p\n",
    "\n",
    "        item_text = text[item_position[0]:item_position[1]]\n",
    "\n",
    "        return item_text\n",
    "\n",
    "    # extract text for each section\n",
    "    results = {}\n",
    "\n",
    "    for section in sections:\n",
    "\n",
    "        # ITEM 2: Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
    "        # it ends at ITEM 3\n",
    "        if section == 'mda':\n",
    "            try:\n",
    "                item2_start = re.compile(\"item\\s*2\\s*[\\.\\;\\:\\-\\_]*[\\s\\n]*Man\", re.IGNORECASE)\n",
    "                item2_end = re.compile(\"item\\s*3\\s*[\\.\\;\\:\\-\\_]*[\\s\\n]*Quanti|item\\s*4\\s*\", re.IGNORECASE)\n",
    "                item2_text = extract_text(text, item2_start, item2_end)\n",
    "                results['mda'] = item2_text\n",
    "            except Exception as e:\n",
    "                results['mda'] = ''\n",
    "                print(f'Error extracting ITEM 2: MD&A for {fname}')\n",
    "        \n",
    "        if section == 'qmr':\n",
    "            try:\n",
    "                item7a_start = re.compile(\"item\\s*3\\s*[\\.\\;\\:\\-\\_]*[\\s\\n]*Quanti\", re.IGNORECASE)\n",
    "                item7a_end = re.compile(\"item\\s*4\", re.IGNORECASE)\n",
    "                item7a_text = extract_text(text, item7a_start, item7a_end)\n",
    "                results['qmr'] = item7a_text\n",
    "            except Exception as e:\n",
    "                results['qmr'] = ''\n",
    "                print(f'Error extracting ITEM 7A: for {fname}')\n",
    "\n",
    "\n",
    "    return results"
   ],
   "execution_count":9,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"MqDpwqDU7mil2WvjEoN3PY",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"0HDao4DX0rjCETap9nUZZA"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def get_info_from_file_name(file_path):\n",
    "    info = file_path.replace('_edgar', '').replace('_data', '').replace('.txt', '').split('_')\n",
    "    #['20110328', '10-K', '1403720', '0001469299-11-000174'] \n",
    "    date = info[0]\n",
    "    form_type = info[1]\n",
    "    cik = info[2]\n",
    "    acc_num = info[3]\n",
    "\n",
    "    return date, form_type, cik, acc_num"
   ],
   "execution_count":10,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"W5KUBUTIoQr17G4QbbHdfK",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"CflPwF23GNvgq5uz3Tlu9x"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def preprocess_and_get_BERT(doc_dict,keys):\n",
    "    section_wise_embeddings = {}\n",
    "    batch_size = 16\n",
    "    for key in keys:\n",
    "        # print(key)\n",
    "        embeddings = np.zeros(shape = (768,))\n",
    "        #embeddings['mean_embed'] = np.zeros(shape = (768,))\n",
    "        sent_count = 0\n",
    "        text = doc_dict[key]\n",
    "        if text:\n",
    "            sentences = text.split('\\n')\n",
    "        else:\n",
    "            sentences = []\n",
    "        if len(sentences) == 1:\n",
    "            sentences = sentences[0].split('.')\n",
    "\n",
    "        sentences = [sent for sent in sentences if len(sent.split(' ')) > 3]\n",
    "        for batch_idx in range(0, len(sentences), batch_size):\n",
    "            batch_sentence = sentences[batch_idx : batch_idx + batch_size]\n",
    "            doc_emb = get_embeddings_for_doc(batch_sentence)\n",
    "            # print(doc_emb.shape)\n",
    "            embeddings += doc_emb\n",
    "        section_wise_embeddings[key] = embeddings\n",
    "        if len(sentences) == 0:\n",
    "            continue\n",
    "        section_wise_embeddings[key] \/= len(sentences)\n",
    "\n",
    "    return section_wise_embeddings"
   ],
   "execution_count":11,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"MSy0pyVckUSwXZn5ZVipYr",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"gM0Pdqf7p9iPRhJxkBvRR8"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# sections_10k = ['business', 'risk', 'mda', 'qmr']\n",
    "# sections_10q = ['mda', 'qmr']\n",
    "# emb_types = ['max_pool', 'mean_embed']\n",
    "# parquet_path = '\/data\/workspace_files\/bert_parquet_files\/'\n",
    "# for yr in ['2011', '2012', '2013']:\n",
    "#     #print(yr)\n",
    "#     for qtr in ['QTR1', 'QTR2', 'QTR3', 'QTR4']:\n",
    "#         print(f\"Running Year {yr}, QTR : {qtr}\")\n",
    "#         #print(qtr)\n",
    "#         clean_text_folder = '\/data\/workspace_files\/10X_folders\/' + yr + '\/' + qtr\n",
    "#         files = [f for f in os.listdir(clean_text_folder)]\n",
    "#         for idx in range(0, len(files), 500):\n",
    "#             save_name = parquet_path + yr + '_' + qtr + '_' + str(idx) + '.parquet'\n",
    "#             if os.path.exists(save_name):\n",
    "#                 print(f\"Exists : {save_name}\")\n",
    "#                 continue\n",
    "#             df_list = []\n",
    "#             pbar = tqdm(files[idx:idx + 500])\n",
    "#             for file_path in pbar:\n",
    "#                 # print(file_path)\n",
    "#                 date, form_type, cik, acc_num = get_info_from_file_name(file_path)\n",
    "#                 if form_type != '10-K' and form_type != '10-Q':\n",
    "#                     continue\n",
    "#                 file_path_req = clean_text_folder + '\/' + file_path\n",
    "\n",
    "#                 if form_type == '10-K':\n",
    "#                     doc_dict = get_itemized_10k(file_path_req)\n",
    "#                     keys = copy.deepcopy(sections_10k)\n",
    "#                 else:\n",
    "#                     doc_dict = get_itemized_10q(file_path_req)\n",
    "#                     keys = copy.deepcopy(sections_10q)\n",
    "#                 section_wise_embeddings = preprocess_and_get_BERT(doc_dict,keys)\n",
    "#                 row = {'Date': date, \"Form_Type\": form_type, 'CIK': cik, \"Accession_Number\": acc_num}\n",
    "#                 for key in keys:\n",
    "#                     for emb_type in emb_types:\n",
    "#                         row[key + '_' + emb_type] = section_wise_embeddings[key]\n",
    "#                 df_list.append(row)\n",
    "#             df = pd.DataFrame(df_list)\n",
    "#             df.to_parquet(save_name, index=False)\n",
    "#             print(\"Done for\", save_name)\n",
    "#             #break\n",
    "#         print(\"Done for qtr\", qtr)\n",
    "#         #break\n",
    "#     #break\n",
    "#     print(\"Done for year\", yr)\n",
    "#     #break"
   ],
   "execution_count":12,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"JUWYfc8Vax3DUWf4DKWecq",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"TUls0GGTAQJB3Q3NRvBfky"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# parquet_path = '\/data\/workspace_files\/bert_parquet_files'\n",
    "# file_name = f'{2011}_QTR{1}_{500}.parquet'\n",
    "# file_path_read_req = parquet_path + file_name\n",
    "# test = pq.read_table(file_path_read_req).to_pandas()\n",
    "\n",
    "# parquet_path = '\/data\/workspace_files\/bert_parquet_files\/all\/'\n",
    "# file_name = f'{2012}.parquet'\n",
    "# file_path_read_req = parquet_path + file_name\n",
    "# test = pq.read_table(file_path_read_req).to_pandas()"
   ],
   "execution_count":13,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"ORneaSXZIN1cpKpL43Jvac",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def create_section_dict_year(year, req_columns, section = None):\n",
    "    parquet_path = '\/data\/workspace_files\/bert_parquet_files\/'\n",
    "    if not section is None:\n",
    "        save_name = f'{parquet_path}{section}\/{year}.parquet'\n",
    "    else:\n",
    "        save_name = f'{parquet_path}all\/{year}.parquet'\n",
    "    if os.path.exists(save_name):\n",
    "        print(f\"Exists : {save_name}\")\n",
    "        return \n",
    "    year_df = []\n",
    "    if not section is None:\n",
    "        section_col = f'{section}_max_pool'\n",
    "    for qtr in range(1, 5):\n",
    "        print(f'Running for QTR {qtr}')\n",
    "        for file_idx in range(0, 20000, 500):\n",
    "            file_name = f'{year}_QTR{qtr}_{file_idx}.parquet'\n",
    "            file_path_read_req = parquet_path + file_name\n",
    "            try: \n",
    "                if not section is None:\n",
    "                    df = pq.read_table(file_path_read_req, columns = req_columns + [section_col]).to_pandas()\n",
    "                else:\n",
    "                    df = pq.read_table(file_path_read_req).to_pandas()\n",
    "                    df['all'] = [np.zeros(len(df['mda_max_pool'].iloc[0]))]*len(df)\n",
    "                    for index, row in df.iterrows():\n",
    "                        count = 0\n",
    "                        all_val = 0\n",
    "                        if row['Form_Type'] == '10-Q':\n",
    "                            mean_cols = [ele + '_max_pool' for ele in ['mda', 'qmr']]\n",
    "                        else:\n",
    "                            mean_cols = [ele + '_max_pool' for ele in ['business', 'risk', 'mda', 'qmr']]\n",
    "                        for col in mean_cols:\n",
    "                            if row[col] is None or np.array_equal(row[col], np.zeros(768)):\n",
    "                                continue\n",
    "                            count += 1\n",
    "                            all_val += row[col]\n",
    "                        if count == 0:\n",
    "                            continue\n",
    "                        else:\n",
    "                            df.at[index,'all'] = all_val\/count\n",
    "                    df = df[req_columns + ['all']]\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            year_df.append(df)\n",
    "    \n",
    "    year_df = pd.concat(year_df, axis=0)\n",
    "    year_df.to_parquet(save_name, index=False)\n",
    "    return "
   ],
   "execution_count":14,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"QnO6KMBAQdT23Vy01ZzRZ1",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "parquet_path = '\/data\/workspace_files\/bert_parquet_files\/'\n",
    "file_name = '2011_QTR1_0.parquet'\n",
    "file_path_read_req = parquet_path + file_name\n",
    "test = pq.read_table(file_path_read_req).to_pandas()"
   ],
   "execution_count":15,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"e9msKruzFYVjHJyj1F8QJh",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "req_columns = ['Date', 'Form_Type', 'CIK', 'Accession_Number']\n",
    "for year in range(2011, 2014):\n",
    "    for section in ['business', 'risk', 'mda', 'qmr']:\n",
    "        create_section_dict_year(str(year), req_columns, section=section)\n",
    "    create_section_dict_year(str(year), req_columns)"
   ],
   "execution_count":16,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Exists : \/data\/workspace_files\/bert_parquet_files\/business\/2011.parquet\n",
      "Exists : \/data\/workspace_files\/bert_parquet_files\/risk\/2011.parquet\n",
      "Exists : \/data\/workspace_files\/bert_parquet_files\/mda\/2011.parquet\n",
      "Exists : \/data\/workspace_files\/bert_parquet_files\/qmr\/2011.parquet\n",
      "Running for QTR 1\n",
      "\/data\/workspace_files\/bert_parquet_files\/2011_QTR1_9500.parquet\n",
      "Running for QTR 2\n",
      "\/data\/workspace_files\/bert_parquet_files\/2011_QTR2_11000.parquet\n",
      "Running for QTR 3\n",
      "\/data\/workspace_files\/bert_parquet_files\/2011_QTR3_11000.parquet\n",
      "Running for QTR 4\n",
      "\/data\/workspace_files\/bert_parquet_files\/2011_QTR4_10000.parquet\n",
      "Exists : \/data\/workspace_files\/bert_parquet_files\/business\/2012.parquet\n",
      "Exists : \/data\/workspace_files\/bert_parquet_files\/risk\/2012.parquet\n",
      "Exists : \/data\/workspace_files\/bert_parquet_files\/mda\/2012.parquet\n",
      "Exists : \/data\/workspace_files\/bert_parquet_files\/qmr\/2012.parquet\n",
      "Running for QTR 1\n",
      "\/data\/workspace_files\/bert_parquet_files\/2012_QTR1_9000.parquet\n",
      "Running for QTR 2\n",
      "\/data\/workspace_files\/bert_parquet_files\/2012_QTR2_10500.parquet\n",
      "Running for QTR 3\n",
      "\/data\/workspace_files\/bert_parquet_files\/2012_QTR3_10000.parquet\n",
      "Running for QTR 4\n",
      "\/data\/workspace_files\/bert_parquet_files\/2012_QTR4_9500.parquet\n",
      "Exists : \/data\/workspace_files\/bert_parquet_files\/business\/2013.parquet\n",
      "Exists : \/data\/workspace_files\/bert_parquet_files\/risk\/2013.parquet\n",
      "Exists : \/data\/workspace_files\/bert_parquet_files\/mda\/2013.parquet\n",
      "Exists : \/data\/workspace_files\/bert_parquet_files\/qmr\/2013.parquet\n",
      "Running for QTR 1\n",
      "\/data\/workspace_files\/bert_parquet_files\/2013_QTR1_8000.parquet\n",
      "Running for QTR 2\n",
      "\/data\/workspace_files\/bert_parquet_files\/2013_QTR2_10500.parquet\n",
      "Running for QTR 3\n",
      "\/data\/workspace_files\/bert_parquet_files\/2013_QTR3_9000.parquet\n",
      "Running for QTR 4\n",
      "\/data\/workspace_files\/bert_parquet_files\/2013_QTR4_8500.parquet\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"pTnyHvWGnW2RqvNuT9uAMD",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    
   ],
   "execution_count":0,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"jgem7wmG8oVCX3SW0op2hF",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"REACTIVE",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[
    
   ],
   "report_row_ids":[
    "7NLQSGlxCXaBLkcwLPKcA3",
    "FdQEykBKwaHEICXVqGXXyl",
    "XHslOzuSXXATF7X9WrARGM",
    "jRv40eEutC7TJi2EHfYnLD",
    "0qnEfUBS35JJPxZ7mz2XPB",
    "dbiftE1Oa7getVtHIJVfqG",
    "GIt49i38RR38k3srR7oJlV",
    "H6SvSplKddmjfg1zoOYtQa",
    "0HDao4DX0rjCETap9nUZZA",
    "CflPwF23GNvgq5uz3Tlu9x",
    "gM0Pdqf7p9iPRhJxkBvRR8",
    "TUls0GGTAQJB3Q3NRvBfky",
    "w8X4EPRg9Fzj35MjObjDRk",
    "zZv4XGEX0oc5t6Ho4GbDx0",
    "PbGUOOKirqpbbeaRKIxiel",
    "qwgY0utZaIeyjrGTAJtq5l",
    "bz9Z9xUqlgKParOYp5nFU5",
    "VIrDkPFHadu4MGmDTArE56",
    "5pRBNhNcBO4XR1x32nLiG3",
    "SsUTqbcCWZMJRWijLBIeZ8",
    "pzVZkw1160IniydqV0BKOL",
    "UyvFbkhCnIKz5dhyTo4cJC",
    "rIFJCPcHqnigwWUZX4Mg5G",
    "SgKDmQgklCV1DBc4PnHiwU"
   ],
   "version":3
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}